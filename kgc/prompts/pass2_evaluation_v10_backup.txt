```
Evaluate these {batch_size} extracted relationships using dual-signal analysis.

RELATIONSHIPS TO EVALUATE:
{relationships_json}

For EACH of the {batch_size} relationships above, provide TWO INDEPENDENT evaluations:

## 🎯 V10 GOAL: COMPREHENSIVE KNOWLEDGE EXTRACTION

V10 focuses on extracting ALL valuable factual relationships, not just discourse elements. This means:
- ✅ Bibliographic citations are HIGH VALUE (not "too simple")
- ✅ Categorical definitions are HIGH VALUE (not "too obvious")
- ✅ Compositional relationships are HIGH VALUE (not "too trivial")
- ✅ Functional relationships are HIGH VALUE (not "too basic")
- ✅ Organizational affiliations are HIGH VALUE (not "too boring")

**CRITICAL**: Do NOT penalize valuable factual knowledge as "too simple" or "low value". These relationships are ESSENTIAL for building a comprehensive knowledge graph.

## 📚 BIBLIOGRAPHIC CITATIONS - SPECIAL HANDLING

Bibliographic citations are **ESSENTIAL KNOWLEDGE GRAPH ELEMENTS**. They are NOT "too simple" or "low value".

**ALWAYS score bibliographic relationships highly:**
- (Author, authored, Book) → **p_true: 0.95-1.0**, text_confidence: 0.9-1.0
- (Book, published by, Publisher) → **p_true: 0.95**, text_confidence: 0.9-1.0
- (Book, published in, Year) → **p_true: 0.95**, text_confidence: 0.9-1.0
- (Person, dedicated, Book to Person) → **p_true: 0.90**, text_confidence: 0.8-0.9
- (Person, endorsed, Book) → **p_true: 0.90**, text_confidence: 0.8-0.9

**Examples**:
- (David Suzuki, authored, Sacred Balance) → **p_true: 1.0** (verifiable authorship)
- (Sacred Balance, published by, Greystone) → **p_true: 0.95** (verifiable publication)
- (Sacred Balance, published in, 1997) → **p_true: 0.95** (verifiable year)
- (Aaron Perry, dedicated, Soil Stewardship Handbook to Osha) → **p_true: 0.90** (verifiable dedication)

**Why HIGH VALUE?**
- Bibliographic citations are foundational knowledge graph data
- They establish provenance and attribution
- They connect authors, books, publishers, and ideas
- They are 100% verifiable and concrete

## 1. TEXT SIGNAL (ignore world knowledge)

How clearly does the text state this relationship?

**Score 0.0-1.0 based purely on text clarity:**
- **0.9-1.0**: Text explicitly and clearly states this relationship
- **0.7-0.9**: Text strongly implies this relationship
- **0.5-0.7**: Text suggests this relationship with some inference required
- **0.3-0.5**: Text vaguely hints at this relationship
- **0.0-0.3**: Text doesn't really support this relationship

## 2. KNOWLEDGE SIGNAL (ignore the text)

Is this relationship plausible, verifiable, and concrete given world knowledge?

**Score based on VERIFIABILITY and CONCRETENESS:**

### 🌟 KNOWLEDGE PLAUSIBILITY CALIBRATION (V10 ENHANCED)

### 0.9-1.0: Easily Verifiable Facts ⭐ HIGH VALUE
**Bibliographic citations, authorship, organizational roles, basic factual claims**

**Examples**:
- (Aaron William Perry, authored, Soil Stewardship Handbook) → **1.0** (verifiable authorship)
- (Soil Stewardship Handbook, published by, Y on Earth Community) → **0.95** (verifiable publication)
- (Sacred Balance, published in, 1997) → **0.95** (verifiable year)
- (Y on Earth, is, nonprofit organization) → **0.95** (easily verified)
- (Slovenia, is, country) → **1.0** (basic fact)
- (Adrian Del Caro, authored, Grounding the Nietzsche Rhetoric of Earth) → **1.0** (verifiable)
- (Joel Salatin, wrote foreword for, Soil Stewardship Handbook) → **0.90** (verifiable endorsement)

### 0.7-0.9: Concrete Facts / Minor Uncertainty ⭐ HIGH VALUE
**Categorical definitions, compositional relationships, scientific consensus, well-documented facts**

**Examples**:
- (soil, is-a, complex ecosystem) → **0.80** (scientific consensus)
- (compost, contains, nitrogen) → **0.85** (verifiable composition)
- (biochar, sequesters, carbon) → **0.80** (scientific consensus)
- (cover cropping, enhances, soil structure) → **0.75** (evidence-based functional relationship)
- (Slovenia, is located in, eastern Alpine region) → **0.85** (geographic fact)
- (Y on Earth, hosts, podcast series) → **0.85** (easily verified organizational fact)
- (composting, produces, humus) → **0.80** (verifiable biological process)

### 0.4-0.6: Debatable / Abstract / Soft Claims
**Causal claims with some factual basis but debatable, abstract concepts with empirical support**

**Examples**:
- (soil health, affects, community wellbeing) → **0.5** (causal claim with some evidence)
- (composting, improves, soil quality) → **0.6** (generally accepted, some evidence)
- (Learning about soil, benefits, families) → **0.5** (plausible but vague entities)

### 0.3-0.5: Philosophical Claims / Normative Statements 🔍 PHILOSOPHICAL_CLAIM DETECTION
**Philosophical essence claims, normative prescriptions, value judgments, metaphorical abstractions**

**PHILOSOPHICAL_CLAIM Detection Criterion:**

**Indicators of philosophical claims:**
- **Essence claims**: "X is what it means to be Y" (defining essence or nature)
- **Metaphorical abstractions**: "X is the answer/key/solution" (abstract metaphorical statements)
- **Normative prescriptions**: "we should/must do X" (prescriptive value statements)
- **Value judgments**: "it is essential that" (subjective importance claims)
- **Existential definitions**: "to be X is to Y" (defining existence or being)

**Scoring impact**: If philosophical claim detected → **p_true: 0.3-0.5** (low confidence, not verifiable)

**Classification**: Add flag **PHILOSOPHICAL_CLAIM**

**Examples**:
- (being connected to land, is what it means to be, human) → **0.3** (philosophical essence claim)
- (soil, is, the answer) → **0.4** (metaphorical abstraction)
- (we, should practice, regenerative agriculture) → **0.4** (normative prescription)
- (it is essential that, we reconnect with, nature) → **0.4** (value judgment)
- (to be human, is to, care for earth) → **0.3** (existential definition)

**Contrast with factual claims:**
- (soil, contains, carbon) → **0.9** (factual, verifiable composition)
- (Aaron Perry, authored, Handbook) → **1.0** (factual, verifiable authorship)
- (composting, produces, humus) → **0.8** (factual, verifiable process)

**Why lower scores for philosophical claims?**
- Not empirically verifiable
- Express subjective values or beliefs
- Define essence/meaning rather than state facts
- Cannot be proven true or false through evidence

### 0.0-0.3: Unverifiable / Metaphorical
**Metaphors, subjective definitions, abstract opinions**

**Examples**:
- (Our land, is, a veritable Eden) → **0.3** (metaphor, not literal fact)
- (the crossroads, is characterized by, immense complexity) → **0.2** (abstract metaphor)

## ENTITY TYPE ASSESSMENT

For the knowledge signal, also consider:
- **What types are the source and target entities?**
- Are they concrete (people, places, organizations, books) or abstract (concepts, metaphors)?
- Concrete entities → higher knowledge scores
- Abstract entities → lower knowledge scores

**Concrete entity types** (prefer 0.7-1.0 scores):
- People (authors, scientists, farmers)
- Organizations (publishers, nonprofits, institutions)
- Books, papers, articles
- Places (countries, regions, farms)
- Materials (soil, compost, biochar)

**Abstract entity types** (typically 0.1-0.5 scores):
- Philosophical concepts (spiritual flourishing, human essence)
- Metaphors (veritable Eden, the crossroads)
- Vague groups (we, they, thousands)

## STATEMENT CLASSIFICATION

Classify each relationship with appropriate flags based on its nature:

### Classification Categories:

**FACTUAL** (p_true: 0.7-1.0):
- Verifiable facts, authorship, organizational relationships
- Bibliographic citations (authored, published by, published in)
- Categorical definitions (is-a, is)
- Compositional relationships (contains, includes, provides)
- Example: (Aaron William Perry, authored, Soil Stewardship Handbook)
- Example: (soil, is-a, complex ecosystem)
- Example: (compost, contains, nitrogen)

**TESTABLE_CLAIM** (p_true: 0.4-0.9):
- Assertions that evidence could support or oppose
- Functional relationships with empirical basis
- Example: (composting, improves, soil quality)
- Example: (cover cropping, prevents, erosion)
- Example: (spiritual practices, enhance, wellbeing)

**PHILOSOPHICAL_CLAIM** (p_true: 0.3-0.5):
- Existential/definitional statements not testable by evidence
- Essence claims ("X is what it means to be Y")
- Metaphorical abstractions ("X is the answer")
- Normative prescriptions ("we should/must do X")
- Value judgments ("it is essential that")
- Example: (being connected to soil, is what it means to be, human)
- Example: (soil, is, the answer)
- Example: (we, should practice, regenerative agriculture)

**METAPHOR** (p_true: 0.1-0.4):
- Figurative language, poetic comparisons
- Example: (our land, is, veritable Eden)
- Example: (the crossroads, is characterized by, complexity)

**OPINION** (p_true: 0.3-0.6):
- Subjective viewpoints, preferences, beliefs
- Different from TESTABLE_CLAIM when no evidence path exists

**ABSTRACT_CONCEPT** (p_true: 0.2-0.5):
- Relationships involving highly abstract entities
- May overlap with other flags

### Multiple Flags:
A relationship can have multiple classification flags:
- (spiritual practices, enhance, wellbeing) → [TESTABLE_CLAIM, ABSTRACT_CONCEPT]
- (being connected to soil, is what it means to be, human) → [PHILOSOPHICAL_CLAIM, ABSTRACT_CONCEPT]
- (Slovenia, is, veritable Eden) → [METAPHOR]
- (Aaron Perry, authored, Handbook) → [FACTUAL]
- (soil, is-a, complex ecosystem) → [FACTUAL]
- (compost, contains, nitrogen) → [FACTUAL]

## CONFLICT DETECTION

Set `signals_conflict=true` when signals diverge significantly:

### Case 1: High Text, Low Knowledge (>0.7 text, <0.4 knowledge)
**Example**: Text clearly states a metaphor, but it's not literally true
- Text: "Our land has remained a veritable Eden" → text_confidence: 0.9
- Knowledge: This is a metaphor, not literal → p_true: 0.3
- **Conflict**: "The text clearly says it, but 'veritable Eden' is a metaphor, not a factual description"

### Case 2: Low Text, High Knowledge (<0.4 text, >0.7 knowledge)
**Example**: Text is vague but the relationship is a well-known fact
- Text: Vague reference to a well-known fact → text_confidence: 0.3
- Knowledge: We know this is true → p_true: 0.8
- **Conflict**: "Text doesn't clearly state it, but we know from world knowledge it's true"

### Case 3: Philosophical/Abstract Despite Clear Text
**Example**: "being connected to soil is what it means to be human"
- Text: Sentence clearly states this → text_confidence: 0.9
- Knowledge: This is a philosophical claim, not a fact → p_true: 0.3
- **Conflict**: "Text is clear, but this is a subjective philosophical claim, not a verifiable fact"

### Case 4: Pronoun/Vague Entity Issues
**Example**: "we can connect with the living soil"
- Text: Sentence states this → text_confidence: 0.7
- Knowledge: "we" is too vague, not a concrete entity → p_true: 0.4
- **Conflict**: "Text states the relationship, but 'we' is a pronoun that needs resolution to a specific entity"

**When setting signals_conflict=true:**
- **MUST include** `conflict_explanation` describing WHY the signals conflict
- **SHOULD include** `suggested_correction` if you know how to fix it

**Examples of conflict_explanation**:
- "Text clearly states metaphor 'veritable Eden', but this is figurative language, not literal fact"
- "Philosophical definition ('what it means to be human') - not verifiable or concrete"
- "Source is pronoun 'we' which needs resolution to specific entity like 'humanity' or 'individuals'"
- "'spiritual flourishing' is an abstract concept - relationship is opinion/belief, not verifiable fact"
- "Source is vague quantifier 'thousands' - should be 'thousands of people' or 'thousands of farmers'"
- "Philosophical essence claim - not empirically verifiable"
- "Normative prescription ('should/must') - expresses value judgment, not factual relationship"

## OUTPUT FORMAT

For each relationship, return:
```json
{
  "candidate_uid": "<UNCHANGED from input>",
  "text_confidence": 0.0-1.0,
  "p_true": 0.0-1.0,
  "signals_conflict": true/false,
  "conflict_explanation": "string (if signals_conflict=true)",
  "suggested_correction": {"source": "...", "relationship": "...", "target": "..."} (optional),
  "source_type": "entity type",
  "target_type": "entity type",
  "classification_flags": ["FACTUAL" | "TESTABLE_CLAIM" | "PHILOSOPHICAL_CLAIM" | "METAPHOR" | "OPINION" | "ABSTRACT_CONCEPT"]
}
```

**classification_flags** should contain all applicable flags for the relationship.

## CRITICAL REMINDERS

1. **Return candidate_uid UNCHANGED** in every output object
2. **Evaluate all {batch_size} relationships** in the same order as input
3. **Assign classification flags** based on statement type (FACTUAL, METAPHOR, PHILOSOPHICAL_CLAIM, etc.)
4. **Knowledge scores reflect verifiability** - philosophical statements and metaphors score 0.0-0.5, but we still extract them
5. **Set signals_conflict=true** when text and knowledge diverge significantly (>0.3 difference)
6. **Always provide conflict_explanation** when signals_conflict=true
7. **⭐ BIBLIOGRAPHIC CITATIONS ARE HIGH VALUE** - score them 0.9-1.0, do NOT dismiss as "too simple"
8. **⭐ CATEGORICAL DEFINITIONS ARE HIGH VALUE** - score them 0.7-0.9, do NOT dismiss as "too obvious"
9. **⭐ COMPOSITIONAL RELATIONSHIPS ARE HIGH VALUE** - score them 0.7-0.9, do NOT dismiss as "too trivial"
10. **⭐ FUNCTIONAL RELATIONSHIPS ARE HIGH VALUE** - score them 0.7-0.9, do NOT dismiss as "too basic"
11. **🔍 DETECT PHILOSOPHICAL CLAIMS** - flag essence claims, normative prescriptions, value judgments with p_true: 0.3-0.5
```